{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T06:36:12.218011200Z",
     "start_time": "2023-07-25T06:36:09.838028100Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T06:36:12.233562Z",
     "start_time": "2023-07-25T06:36:12.222012Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, kernelSize = 3, inChannels = 64, outChannels = 64, strd = 1, paddng = 1):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = inChannels, out_channels = outChannels, kernel_size = kernelSize, stride = strd, padding = paddng),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(in_channels = inChannels, out_channels = outChannels, kernel_size = kernelSize, stride = strd, padding = paddng),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return torch.add(out, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T06:36:12.275873500Z",
     "start_time": "2023-07-25T06:36:12.234577200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, inChannels,scaleFactor):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels= inChannels, out_channels= inChannels * scaleFactor ** 2, kernel_size=3, stride=1, padding=1)\n",
    "        self.ps = nn.PixelShuffle(scaleFactor)\n",
    "        self.act = nn.PReLU(inChannels)\n",
    "    def forward(self, x):\n",
    "        return self.act(self.ps(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T06:36:12.276872800Z",
     "start_time": "2023-07-25T06:36:12.255708200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SRResnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRResnet, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Conv2d(kernel_size=9, stride=1, in_channels=3, out_channels=64, padding=4)\n",
    "        self.l2 = nn.PReLU()\n",
    "\n",
    "        self.residuals = nn.Sequential()\n",
    "        for _ in range(0, 8):\n",
    "            self.residuals.add_module('residualBlock',ResidualBlock())\n",
    "\n",
    "        self.postResiduals = nn.Sequential(\n",
    "            nn.Conv2d(in_channels= 64, out_channels=64, kernel_size= 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "\n",
    "        self.upsample = UpsampleBlock(64, 2)\n",
    "        self.upsample2 = UpsampleBlock(64, 2)\n",
    "\n",
    "        self.final = nn.Conv2d(64, 3, kernel_size= 9, stride=1, padding=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x1 = self.l2(x)\n",
    "        x = self.residuals(x1)\n",
    "        x = self.postResiduals(x)\n",
    "        x = torch.add(x, x1)\n",
    "        x = self.upsample(x)\n",
    "        x = self.upsample2(x)\n",
    "        x = self.final(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T06:36:13.616896Z",
     "start_time": "2023-07-25T06:36:12.272344100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the path to your data folder\n",
    "\n",
    "data_path = \"dataset/\"\n",
    "\n",
    "# Define the transformations for your data\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop((256, 256)),  # Resize the images to a fixed size\n",
    "    transforms.ToTensor(),          # Convert images to tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize image pixels to the range [-1, 1]\n",
    "])\n",
    "transformLr = transforms.Compose([\n",
    "    transforms.CenterCrop((256, 256)),\n",
    "    transforms.Resize((64,64)), # Resize the images to a fixed size\n",
    "    transforms.ToTensor(),          # Convert images to tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize image pixels to the range [-1, 1]\n",
    "])\n",
    "\n",
    "\n",
    "# Load the high-resolution and low-resolution images\n",
    "hr_dataset = ImageFolder(root=data_path + \"lr\", transform=transform)\n",
    "lr_dataset = ImageFolder(root=data_path + \"lr\", transform=transformLr)\n",
    "#sr_dataset = ImageFolder(root=data_path + \"autoencodertrain\", transform=transform)\n",
    "\n",
    "# Create the data loader for high-resolution and low-resolution images\n",
    "batch_size = 50\n",
    "num_workers = 2  # Set the number of worker processes for data loading\n",
    "hr_data_loader = DataLoader(hr_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "lr_data_loader = DataLoader(lr_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T06:36:13.774453700Z",
     "start_time": "2023-07-25T06:36:13.616896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Carrega Rede\n",
    "srresnet = SRResnet()\n",
    "#srresnet.load_state_dict(torch.load('Model_Snapshots/Gen/Modelo_atual.pt'))\n",
    "\n",
    "# Move the models to the device\n",
    "srresnet.to(device)\n",
    "\n",
    "content_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T06:37:07.025442500Z",
     "start_time": "2023-07-25T06:36:13.774453700Z"
    },
    "collapsed": false
   },
   "source": [
    "test_path = \"Testes/\"\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "test_dataset = ImageFolder(root=test_path, transform= test_transform)\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "test_data_loader = DataLoader(test_dataset, batch_size= batch_size, num_workers= num_workers)\n",
    "\n",
    "i = 1\n",
    "for image in test_data_loader:\n",
    "    sr_images = srresnet(image[0].float().to(device))\n",
    "    save_image(sr_images, f\"{i}.png\", normalize = True)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the optimizers for generator and discriminator\n",
    "lr = 0.001\n",
    "betas = (0.5, 0.9)\n",
    "optimizer_gen = optim.Adam(srresnet.parameters(), lr = lr, betas = betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T19:29:22.804350800Z",
     "start_time": "2023-07-16T19:29:22.761406600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 150\n",
    "save_interval = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T19:29:22.812311100Z",
     "start_time": "2023-07-16T19:29:22.777966400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arrayError = np.zeros(shape=save_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 10\n",
    "step = factor / 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T19:30:12.013571Z",
     "start_time": "2023-07-16T19:29:22.804350800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SmcGu\\anaconda3\\envs\\torchinho\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([50, 3, 256, 256])) that is different to the input size (torch.Size([50, 3, 512, 512])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (256) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Calculating error\u001b[39;00m\n\u001b[0;32m     13\u001b[0m sr_images \u001b[38;5;241m=\u001b[39m srresnet(lr_images)\n\u001b[1;32m---> 14\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcontent_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43msr_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhr_images\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#* factor\u001b[39;00m\n\u001b[0;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     16\u001b[0m optimizer_gen\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\SmcGu\\anaconda3\\envs\\torchinho\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\SmcGu\\anaconda3\\envs\\torchinho\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SmcGu\\anaconda3\\envs\\torchinho\\lib\\site-packages\\torch\\nn\\functional.py:3294\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3292\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3294\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32mc:\\Users\\SmcGu\\anaconda3\\envs\\torchinho\\lib\\site-packages\\torch\\functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (256) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(0, num_epochs):\n",
    "    for i, (hr_images, lr_images) in enumerate(zip(hr_data_loader, lr_data_loader)):\n",
    "        #factor -= step\n",
    "        # Move images to the device\n",
    "        hr_images = hr_images[0].to(device).float()\n",
    "        lr_images = lr_images[0].to(device).float()\n",
    "\n",
    "        srresnet.zero_grad()\n",
    "        optimizer_gen.zero_grad()\n",
    "\n",
    "        # Calculating error\n",
    "        sr_images = srresnet(lr_images)\n",
    "        loss = content_loss(sr_images, hr_images) #* factor\n",
    "        loss.backward()\n",
    "        optimizer_gen.step()\n",
    "\n",
    "        arrayError[i % save_interval] = loss.item()\n",
    "\n",
    "        # Print progress\n",
    "        #if(i + 1) % 10 == 0:\n",
    "            #print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(hr_data_loader)}],\" f\"Generator Loss: {loss.item():.4f}. \")\n",
    "        # Print mean\n",
    "        if i % save_interval == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(hr_data_loader)}], \" f\"Mean Loss: {arrayError.mean():.6f}. \", f\"Factor: {factor}\")\n",
    "\n",
    "        if (i + 1) % save_interval == 0:\n",
    "            torch.save(srresnet.state_dict(), f\"Model_Snapshots/Gen/srresnet_model_epoch{epoch+1}_batch{i+1}.pt\")\n",
    "            print(f\"Saved SRResNet model and images at epoch {epoch+1}, batch {i+1}.\")\n",
    "\n",
    "            #save_image(sr_images, f\"AE-results/sr_image_epoch{epoch + 1}_batch{i+1}.png\", normalize = True)\n",
    "            save_image(sr_images, f\"Net_Results/image_epoch{epoch + 1}_batch{i+1}_sr.png\", normalize = True)\n",
    "            save_image(hr_images, f\"Net_Results/image_epoch{epoch + 1}_batch{i+1}_hr.png\", normalize = True)\n",
    "            save_image(lr_images, f\"Net_Results/image_epoch{epoch + 1}_batch{i+1}_lr.png\", normalize = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
